# MicDrop -- A Mini NotebookLM Powered by GPT

Meet **MicDrop** -- a mini NotebookLM that turns your short text documents into intuitive, conversational podcasts.

## ðŸš§ Project Evolution

This repo captures the two stages of MicDrop's development -- both powered by synthetic datasets, but with significant differences in scale and design:

- **Version 1 - Proof-of-Concept**: A fast prototype using a small, synthetic dataset to test the pipeline for transforming articles into podcasts. |
  View the notebook: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/177J6947KLG3UtF3ZdulVNKG8FaHvZQYS?usp=sharing) |
- **Version 2 - Enhanced Workflow**: A more advanced implementation using a scaled-up synthetic dataset generated by GPT-4o. This version introduces a cleaner, more modular workflow and sets the foundation for robust model training. | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/101MJhSnomGYCt2G0xmW8XN0IT9kNHPVk) |

## Project Overview

Inspired by Google's cutting-edge AI tool and powered by not one but two GPT models, MicDrop is a proof-of-concept, ambitious prototype that aims to take information-rich documents and make them into engaging audio.

To accomplish this text-generation task -- and ultimately transform these scripts into MicDrop episodes -- we will begin by fine-tuning `distilgpt2`, the streamlined version of GPT-2, which is smaller and faster but still effective. To learn more about this large language model check out the official model card on [Hugging Face](https://huggingface.co/distilgpt2).

Our `distilgpt2` model will be fine-tuned on an initial dataset consisting of 200 pairings of articles and corresponding podcast scripts. Each of articles is 250-300 words and each of the scripts -- the foundation for our bite-sized episodes -- are 450-600 words.

But you might be wondering, where will this dataset come from?

We'll build it -- from scratch.

To do so, we will utilize a teacher LLM -- specifically GPT-4o -- to generate the synthetic dataset that its predecessor `distilgpt2` will learn from.

Finally, we will bring these scripts to life with a text-to-speech tool.

Ok. So. Stay tuned.

## The Pipeline

Before we *record* our first episode, here's a roadmap of what is to come:

- Set up the GPU runtime
- Generate the synthetic dataset with GPT-4o, the *teacher* LLM
- Prepare the dataset for `distilgpt2`
- Load the pretrained `distilgpt2` model
- Test the base model on a single sentence
- Fine-tune the model using the article-podcast script dataset
- Test the fine-tuned model's podcast script-writing skills
- Transform the scripts into MicDrop episodes
- The next step

## Tools & Libraries Used

- **Google Colab** â€” for free GPU access and seamless integration with Drive  
- **Python 3.11.12** â€” base language for all modeling and data wrangling   
- **transformers** â€” [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) library for working with `distilgpt2` and custom fine-tuning.
- **torch** â€” PyTorch backend powering the fine-tuning and inference  
- **curl** â€” to download the custom training script from the course server    
- **subprocess** â€” to manage training script execution with more readable output  
- **Google Drive** â€” used to store and export the final model
- **gdown** - to download the fine-tuned model ZIP file from Google Drive
- **zipfile & json** - to unzip the downloaded model and patch its config so it runs correctly

## Acknowledgements

A big thanks to Michele Samorani, Associate Professor in the Department of Information Systems & Analytics at Santa Clara University. My work in his NLP course at the Leavey School of Business was what sparked the idea to build MicDrop. And thanks to him for calling this: "One of the coolest projects I've seen."
