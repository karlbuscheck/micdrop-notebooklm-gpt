# MicDrop -- A Mini NotebookLM Powered by GPT

Meet **MicDrop** -- a mini NotebookLM that turns your short text documents into intuitive, conversational podcasts.

## ðŸš§ Project Evolution

This repo captures the two stages of MicDrop's development:

- **Version 1 - Proof-of-Concept**: A fast prototype built to test the pipeline for transforming text documents into podcasts. | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/177J6947KLG3UtF3ZdulVNKG8FaHvZQYS?usp=sharing) |
- **Version 2 - Enhanced Workflow**: A more advanced version featuring a scaled synthetic dataset generated by GPT-4o. | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/101MJhSnomGYCt2G0xmW8XN0IT9kNHPVk) |

## Project Overview

Inspired by Google's cutting-edge AI tool and powered by not one but two GPT models, MicDrop is a proof-of-concept, ambitious prototype that aims to take information-rich documents and make them into engaging audio.

To accomplish this text-generation task -- and ultimately transform these scripts into MicDrop episodes -- we will begin by fine-tuning `distilgpt2`, the streamlined version of GPT-2, which is smaller and faster but still effective. To learn more about this large language model check out the official model card on [distilgpt2 model card on Hugging Face](https://huggingface.co/distilgpt2)

Our `distilgpt2` model will be fine-tuned on an initial dataset consisting of 200 pairings of articles and corresponding podcast scripts. Each of articles is 250-300 words and each of the scripts -- the foundation for our bite-sized episodes -- are 450-600 words.

But you might be wondering, where will this dataset come from?

We'll build it -- from scratch.

To do so, we will utilize a teacher LLM -- specifically GPT-4o -- to generate the synthetic dataset that its predecessor `distilgpt2` will learn from.

Finally, we will bring these scripts to life with a text-to-speech tool.

Ok. So. Stay tuned.

## The Pipeline

Before we *record* our first episode, here's a roadmap of what is to come:

- Set up the GPU runtime
- Generate the synthetic dataset with GPT-4o, the *teacher* LLM
- Prepare the dataset for `distilgpt2`
- Load the pretrained `distilgpt2` model
- Test the base model on a single sentence
- Fine-tune the model using the article-podcast script dataset
- Test the fine-tuned model's podcast script-writing skills
- Transform the scripts into MicDrop episodes
- The next step

## Tools & Libraries Used

- **Google Colab** â€” for free GPU access and seamless integration with Drive  
- **Python 3.11.12** â€” base language for all modeling and data wrangling  
- **pandas** â€” for reading and managing the dataset of 120K recipes  
- **transformers** â€” Hugging Face's library for working with 'distilgpt2` and custom fine-tuning  
- **torch** â€” PyTorch backend powering the fine-tuning and inference  
- **curl** â€” to download the custom training script from the course server  
- **shutil** â€” for saving and exporting the fine-tuned model  
- **subprocess** â€” to manage training script execution with more readable output  
- **Google Drive** â€” used to store and export the final model

## Acknowledgements

A big thanks to Michele Samorani, Associate Professor in the Department of Information Systems & Analytics at Santa Clara University. My work in his NLP course at the Leavey School of Business was what sparked the idea to build MicDrop.
